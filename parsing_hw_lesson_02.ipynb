{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание\n",
    "#### Урок 2. Парсинг HTML. BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариант 1\n",
    "Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайтов Superjob и HH. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:\n",
    "Наименование вакансии.\n",
    "Предлагаемую зарплату (отдельно минимальную и максимальную).\n",
    "Ссылку на саму вакансию.\n",
    "Сайт, откуда собрана вакансия. ### По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import lxml\n",
    "import re\n",
    "import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hascyr(s):\n",
    "    lower = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
    "    return lower.intersection(s.lower())!=set(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cyrillic(text):\n",
    "    return bool(re.search('[а-яА-Я]', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hh(url, headers):    \n",
    "    response = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(response, 'lxml')    \n",
    "    \n",
    "    v_name_soup = 'body ' + 'div.vacancy-serp-item__info ' + 'span ' * 3 + 'a'    #'div ' * 16 + \n",
    "    v_link_soup = v_name_soup + ' href'\n",
    "    v_sum_soup = 'body ' + 'div.vacancy-serp-item__sidebar ' + 'span' #'div ' * 15 +\n",
    "    v_firm_soup = 'body ' +  'div.vacancy-serp-item__meta-info-company '+ 'a' #'div ' * 18 +\n",
    "    v_adress_soup = 'body ' +  'div.vacancy-serp-item__meta-info ' + 'span.vacancy-serp-item__meta-info' #'div ' * 18 +\n",
    "    \n",
    "    v_name = [i.text for i in soup.select(v_name_soup)]    \n",
    "    v_link = [i['href'] for i in soup.select(v_name_soup)]    \n",
    "    \n",
    "    v_firm = [i.text for i in soup.select(v_firm_soup)]    \n",
    "    v_adress = [i.text for i in soup.select(v_adress_soup)]    \n",
    "    \n",
    "    data_to_return = []\n",
    "    for num, i in enumerate(v_name):\n",
    "        data_to_return.append({\n",
    "            \"name\": v_name[num],\n",
    "            \"link\": v_link[num],\n",
    "            \"sum_min\": 0, #v_sum[num].replace('\\u202f', ''),\n",
    "            \"sum_max\": 0, #v_sum[num].replace('\\u202f', ''),\n",
    "            \"firm\": v_firm[num],\n",
    "            \"adress\": v_adress[num],\n",
    "            \"site\": 'hh.ru'#,\n",
    "            #\"text\":''\n",
    "        })  \n",
    "    \n",
    "    return data_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_hh_sum(url, headers):    \n",
    "    response = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(response, 'lxml') \n",
    "    txt=[i.text for i in soup.select('div.vacancy-description')]\n",
    "    find_min = response.find('vac_salary_from')\n",
    "    findmax = response.find('vac_salary_to')\n",
    "    findCur = response.find('vac_salary_cur')\n",
    "    strmin =response[find_min : findmax]\n",
    "    s_min=strmin[(strmin.index(\":\")+3):(strmin.index(\"\\n\")-2)]\n",
    "    if len(s_min)==0:\n",
    "        s_min=0\n",
    "    else:\n",
    "        s_min=int(s_min)\n",
    "    strmax=response[findmax : findCur]\n",
    "    s_max=strmax[(strmax.index(\":\")+3):(strmax.index(\"\\n\")-2)]\n",
    "    if len(s_max)==0:\n",
    "        s_max=0\n",
    "    else:\n",
    "        s_max=int(s_max)\n",
    "    return s_min,s_max #, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какую вакансию ищем:data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vacancy = input(\"Какую вакансию ищем:\") #find_vacancy = 'data+scientist'\n",
    "find_vacancy_hh = text_vacancy.replace(' ', '+')\n",
    "find_vacancy_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сколько страниц смотрим 1- 5:2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = int(input(\"Сколько страниц смотрим 1- 5:\"))\n",
    "col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hh.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hh=[]\n",
    "for i in range(1, col_list+1):    \n",
    "    url = 'https://www.hh.ru/search/vacancy?clusters=true&area={}&enable_snippets=true&salary=&st=searchVacancy&text='.format(i) + find_vacancy_hh\n",
    "    #ссылка с ценой\n",
    "    #'https://hh.ru/search/vacancy?clusters=true&enable_snippets=true&st=searchVacancy&text=data++scientist&area=1&salary=205000&only_with_salary=true'\n",
    "    #print(url)\n",
    "   \n",
    "    hh_temp = parse_hh(url, headers)    \n",
    "    for j in hh_temp:\n",
    "        url2=j['link']\n",
    "        #print(url2)\n",
    "        j['sum_min'], j['sum_max'] = parse_hh_sum(url2, headers)\n",
    "        \n",
    "    all_hh += hh_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_hh).to_csv('vacancy_hh.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_hh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find superjob.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sj(url, headers):\n",
    "    response = requests.get(url, headers=headers).text\n",
    "    soup = BeautifulSoup(response, 'lxml')   \n",
    "    \n",
    "    data_to_return_sj =[]   \n",
    "    #найдено вакансий\n",
    "    for i in soup.findAll('span', {'class':\"_1h3Zg _2xFS1 _2hCDz _2ZsgW\"}):\n",
    "        print(i.text)\n",
    "    # ссылки на вакансии\n",
    "    for j in soup.findAll('div', {'class':\"_1h3Zg _2rfUm _2hCDz _21a7u\"}):\n",
    "        #print(j.a)    \n",
    "        #print(\"www.superjob.ru\" + j.a['href'])    \n",
    "        data_to_return_sj.append({\n",
    "            \"name\": \"\",\n",
    "            \"link\": \"https://www.superjob.ru\" + j.a['href'],\n",
    "            \"sum_min\": 0, \n",
    "            \"sum_max\": 0, \n",
    "            \"firm\": \"\",\n",
    "            \"adress\": \"\",\n",
    "            \"site\": 'superjob.ru'#,\n",
    "            #\"text\":''\n",
    "        }) \n",
    "    \n",
    "            \n",
    "    return data_to_return_sj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "https://www.superjob.ru/vacancy/search/?keywords=data&geo%5Bt%5D%5B0%5D=4\n",
      "Найдено 59 вакансий\n",
      "https://www.superjob.ru/vacancy/search/?keywords=data&geo%5Bt%5D%5B0%5D=4&page=2\n",
      "Найдено 59 вакансий\n"
     ]
    }
   ],
   "source": [
    "find_vacancy_sj = text_vacancy.replace(' ', '-')\n",
    "if has_cyrillic(find_vacancy_sj)== True:\n",
    "    find_vacancy_sj = transliterate.translit(find_vacancy_sj, reversed=True)\n",
    "print(find_vacancy_sj)\n",
    "\n",
    "all_sj=[]\n",
    "\n",
    "for i in range(1, col_list+1):    \n",
    "    if i == 1:\n",
    "        url = f'https://www.superjob.ru/vacancy/search/?keywords={find_vacancy_sj}&geo%5Bt%5D%5B0%5D=4'\n",
    "    else:\n",
    "        url = f'https://www.superjob.ru/vacancy/search/?keywords={find_vacancy_sj}&geo%5Bt%5D%5B0%5D=4&page={i}'\n",
    "    \n",
    "    #https://www.superjob.ru/vacancy/search/?keywords=data&geo%5Bt%5D%5B0%5D=4\n",
    "    #https://www.superjob.ru/vacancy/search/?keywords=data&geo%5Bt%5D%5B0%5D=4&page=2\n",
    "    \n",
    "    print(url)\n",
    "   \n",
    "    sj_temp = parse_sj(url, headers)        \n",
    "    \n",
    "    #заполнение данными\n",
    "    for j in sj_temp:\n",
    "        url2=j['link']        \n",
    "        \n",
    "        response = requests.get(url2, headers=headers).text\n",
    "        soup = BeautifulSoup(response, 'lxml')       \n",
    "        #название вакансии\n",
    "        for d in soup.findAll('h1', {'class':\"_1h3Zg rFbjy _2dazi _2hCDz\"}):\n",
    "            j['name'] = (d.text)            \n",
    "                \n",
    "        #местонахождение\n",
    "        for d in soup.findAll('span', {\"_6-z9f\"}):\n",
    "            j['adress'] = (d.span.text)        \n",
    "        \n",
    "        #оклад\n",
    "        salary = \"\"\n",
    "        for d1 in soup.findAll('span', {'class':\"_1h3Zg _2Wp8I _2rfUm _2hCDz\"}):\n",
    "            salary += d1.text\n",
    "            \n",
    "        salary=salary.replace(u'\\xa0', u'')\n",
    "        \n",
    "        if '—' in salary:\n",
    "                    salary_min = salary.split('—')[0]\n",
    "                    salary_min = re.sub(r'[^0-9]', '', salary_min)\n",
    "                    salary_max = salary.split('—')[1]\n",
    "                    salary_max = re.sub(r'[^0-9]', '', salary_max)\n",
    "                    salary_min = int(salary_min)\n",
    "                    salary_max = int(salary_max)\n",
    "        elif 'от' in salary:\n",
    "                    salary_min = salary[2:]\n",
    "                    salary_min = re.sub(r'[^0-9]', '', salary_min)\n",
    "                    salary_min = int(salary_min)\n",
    "                    salary_max = None\n",
    "        elif 'договорённости' in salary:\n",
    "                    salary_min = None\n",
    "                    salary_max = None\n",
    "        elif 'до' in salary:\n",
    "                    salary_min = None\n",
    "                    salary_max = salary[2:]\n",
    "                    salary_max = re.sub(r'[^0-9]', '', salary_max)\n",
    "                    salary_max = int(salary_max)\n",
    "        else:\n",
    "                    salary_min = int(re.sub(r'[^0-9]', '', salary))\n",
    "                    salary_max = int(re.sub(r'[^0-9]', '', salary))    \n",
    "            \n",
    "            \n",
    "        j['sum_min'] = salary_min\n",
    "        j['sum_max'] = salary_max\n",
    "        \n",
    "        #содержание вакансии\n",
    "        #txt=\"\"\n",
    "        #for d2 in soup.findAll('span', {'class':\"_1h3Zg _2LeqZ _1TK9I _2hCDz _2ZsgW _2SvHc\"}):\n",
    "        #    txt+=d2.text         \n",
    "        #j['text'] = txt\n",
    "        \n",
    "        #компания\n",
    "        for d in soup.findAll('h2', {'class':\"_1h3Zg _2rfUm _2hCDz _2ZsgW _21a7u _2SvHc\"}):\n",
    "            j['firm'] = (d.text)\n",
    "            \n",
    "    all_sj += sj_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_sj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Вариант 2\n",
    "Необходимо собрать информацию по продуктам питания с сайтов:\n",
    "Роскачество официальный сайт. Исследование качества продуктов питания | Рейтинг товаров.\n",
    "Список протестированных продуктов на сайте Росконтроль.рф\n",
    "Получившийся список должен содержать:\n",
    "Наименование продукта.\n",
    "Категорию продукта (например «Бакалея»).\n",
    "Подкатегорию продукта (например «Рис круглозерный»).\n",
    "Параметр «Безопасность».\n",
    "Параметр «Качество».\n",
    "Общий балл.\n",
    "Сайт, откуда получена информация. ### Структура должна быть одинаковая для продуктов с обоих сайтов. Общий результат можно вывести с помощью dataFrame через Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_sj).to_csv('vacancy_sj.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
